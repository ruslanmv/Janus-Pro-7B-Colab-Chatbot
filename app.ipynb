{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM1fre7JzFsNhS5Ux+cS6Cm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/Janus-Pro-7B-Colab-Chatbot/blob/master/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "def clone_and_extract(repo_url):\n",
        "    \"\"\"\n",
        "    Clones a Git repository and extracts its contents into the current directory.\n",
        "\n",
        "    Args:\n",
        "    repo_url: The URL of the Git repository to clone.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Clone the repository into a temporary directory\n",
        "        temp_dir = \"temp_repo\"\n",
        "        subprocess.run(['git', 'clone', repo_url, temp_dir], check=True)\n",
        "\n",
        "        # Move the contents of the cloned repository to the current directory\n",
        "        for item in os.listdir(temp_dir):\n",
        "            src = os.path.join(temp_dir, item)\n",
        "            dst = os.path.join('.', item)\n",
        "            shutil.move(src, dst)\n",
        "\n",
        "        # Remove the temporary directory\n",
        "        shutil.rmtree(temp_dir)\n",
        "\n",
        "        print(f\"Successfully cloned and extracted repository: {repo_url}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error cloning or extracting repository: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    repo_url = \"https://github.com/ruslanmv/Janus-Pro-7B-Colab-Chatbot\"\n",
        "    clone_and_extract(repo_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG6X9s0Q1aqv",
        "outputId": "73bbfd05-0e3a-4546-9196-40582a494a6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully cloned and extracted repository: https://github.com/ruslanmv/Janus-Pro-7B-Colab-Chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlqgZsV3yicf",
        "outputId": "c383bf3a-2517-40cc-bf9b-27e3653cd707"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.41.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.23.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "aMmvANHZyV85",
        "outputId": "6f6cd51a-7f0a-4e22-ae93-ee7bcd198014"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'janus.models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d217e0cc30db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjanus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiModalityCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVLChatProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjanus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_pil_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janus.models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
        "from janus.utils.io import load_pil_images\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from Upsample import RealESRGAN\n",
        "# from spaces import GPU\n",
        "\n",
        "# --- Configuration and Model Loading ---\n",
        "model_path = \"deepseek-ai/Janus-Pro-7B\"\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    config = AutoConfig.from_pretrained(model_path)\n",
        "    language_config = config.language_config\n",
        "    language_config._attn_implementation = 'eager'\n",
        "\n",
        "    vl_gpt = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        language_config=language_config,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        vl_gpt = vl_gpt.to(torch.bfloat16).cuda()\n",
        "    else:\n",
        "        vl_gpt = vl_gpt.to(torch.float16)\n",
        "\n",
        "    vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n",
        "    tokenizer = vl_chat_processor.tokenizer\n",
        "\n",
        "    sr_model = RealESRGAN(torch.device('cuda' if torch.cuda.is_available() else 'cpu'), scale=2)\n",
        "    sr_model.load_weights(f'weights/RealESRGAN_x2.pth', download=False)\n",
        "\n",
        "    return vl_gpt, vl_chat_processor, tokenizer, sr_model\n",
        "\n",
        "vl_gpt, vl_chat_processor, tokenizer, sr_model = load_models()\n",
        "cuda_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# --- Helper Functions ---\n",
        "@torch.inference_mode()\n",
        "def multimodal_understanding(image, question, seed, top_p, temperature):\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"<|User|>\",\n",
        "            \"content\": f\"<image_placeholder>\\n{question}\",\n",
        "            \"images\": [image],\n",
        "        },\n",
        "        {\"role\": \"<|Assistant|>\", \"content\": \"\"},\n",
        "    ]\n",
        "\n",
        "    pil_images = [Image.fromarray(image)]\n",
        "    prepare_inputs = vl_chat_processor(\n",
        "        conversations=conversation, images=pil_images, force_batchify=True\n",
        "    ).to(cuda_device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16)\n",
        "\n",
        "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
        "    outputs = vl_gpt.language_model.generate(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=prepare_inputs.attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False if temperature == 0 else True,\n",
        "        use_cache=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(input_ids,\n",
        "             width,\n",
        "             height,\n",
        "             temperature: float = 1,\n",
        "             parallel_size: int = 5,\n",
        "             cfg_weight: float = 5,\n",
        "             image_token_num_per_image: int = 576,\n",
        "             patch_size: int = 16):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    tokens = torch.zeros((parallel_size * 2, len(input_ids)), dtype=torch.int).to(cuda_device)\n",
        "    for i in range(parallel_size * 2):\n",
        "        tokens[i, :] = input_ids\n",
        "        if i % 2 != 0:\n",
        "            tokens[i, 1:-1] = vl_chat_processor.pad_id\n",
        "\n",
        "    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(tokens)\n",
        "    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).to(cuda_device)\n",
        "    pkv = None\n",
        "\n",
        "    for i in range(image_token_num_per_image):\n",
        "        with torch.no_grad():\n",
        "            outputs = vl_gpt.language_model.model(inputs_embeds=inputs_embeds,\n",
        "                                                use_cache=True,\n",
        "                                                past_key_values=pkv)\n",
        "            pkv = outputs.past_key_values\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "            logits = vl_gpt.gen_head(hidden_states[:, -1, :])\n",
        "            logit_cond = logits[0::2, :]\n",
        "            logit_uncond = logits[1::2, :]\n",
        "            logits = logit_uncond + cfg_weight * (logit_cond - logit_uncond)\n",
        "            probs = torch.softmax(logits / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            generated_tokens[:, i] = next_token.squeeze(dim=-1)\n",
        "            next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)\n",
        "            img_embeds = vl_gpt.prepare_gen_img_embeds(next_token)\n",
        "            inputs_embeds = img_embeds.unsqueeze(dim=1)\n",
        "\n",
        "    patches = vl_gpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int),\n",
        "                                                 shape=[parallel_size, 8, width // patch_size, height // patch_size])\n",
        "    return generated_tokens.to(dtype=torch.int), patches\n",
        "\n",
        "def unpack(dec, width, height, parallel_size=5):\n",
        "    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)\n",
        "    dec = np.clip((dec + 1) / 2 * 255, 0, 255)\n",
        "    visual_img = np.zeros((parallel_size, width, height, 3), dtype=np.uint8)\n",
        "    visual_img[:, :, :] = dec\n",
        "    return visual_img\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_image(prompt,\n",
        "                   seed=None,\n",
        "                   guidance=5,\n",
        "                   t2i_temperature=1.0):\n",
        "    torch.cuda.empty_cache()\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    width = 384\n",
        "    height = 384\n",
        "    parallel_size = 5\n",
        "\n",
        "    with torch.no_grad():\n",
        "        messages = [{'role': '<|User|>', 'content': prompt},\n",
        "                    {'role': '<|Assistant|>', 'content': ''}]\n",
        "        text = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(\n",
        "            conversations=messages,\n",
        "            sft_format=vl_chat_processor.sft_format,\n",
        "            system_prompt=''\n",
        "        )\n",
        "        text = text + vl_chat_processor.image_start_tag\n",
        "\n",
        "        input_ids = torch.LongTensor(tokenizer.encode(text))\n",
        "        output, patches = generate(input_ids,\n",
        "                                   width // 16 * 16,\n",
        "                                   height // 16 * 16,\n",
        "                                   cfg_weight=guidance,\n",
        "                                   parallel_size=parallel_size,\n",
        "                                   temperature=t2i_temperature)\n",
        "        images = unpack(patches,\n",
        "                        width // 16 * 16,\n",
        "                        height // 16 * 16,\n",
        "                        parallel_size=parallel_size)\n",
        "\n",
        "        stime = time.time()\n",
        "        ret_images = [image_upsample(Image.fromarray(images[i])) for i in range(parallel_size)]\n",
        "        print(f'upsample time: {time.time() - stime}')\n",
        "        return ret_images\n",
        "\n",
        "# @GPU(duration=60)\n",
        "def image_upsample(img: Image.Image) -> Image.Image:\n",
        "    if img is None:\n",
        "        raise Exception(\"Image not uploaded\")\n",
        "    width, height = img.size\n",
        "    if width >= 5000 or height >= 5000:\n",
        "        raise Exception(\"The image is too large.\")\n",
        "    global sr_model\n",
        "    result = sr_model.predict(img.convert('RGB'))\n",
        "    return result\n",
        "\n",
        "# --- Streamlit App ---\n",
        "st.set_page_config(page_title=\"Janus Pro App\", layout=\"wide\")\n",
        "\n",
        "# --- Sidebar ---\n",
        "with st.sidebar:\n",
        "    st.title(\"Janus Pro App\")\n",
        "    st.markdown(\n",
        "        \"This application demonstrates the capabilities of the Janus Pro model, \"\n",
        "        \"including multimodal understanding and text-to-image generation.\"\n",
        "    )\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\n",
        "        \"## Model Parameters\"\n",
        "    )\n",
        "    default_seed = st.sidebar.number_input(\"Seed\", value=42, min_value=0, step=1)\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    st.markdown(\n",
        "        \"Created by [Ruslan Magana Vsevolodovna](https://ruslanmv.com/)\"\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Main Content ---\n",
        "st.title(\"Multimodal Understanding and Text-to-Image Generation with Janus Pro\")\n",
        "\n",
        "# --- Multimodal Understanding Section ---\n",
        "with st.expander(\"**Multimodal Understanding**\", expanded=True):\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        image_input = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "        question_input = st.text_input(\"Enter your question about the image\")\n",
        "\n",
        "    with col2:\n",
        "        top_p = st.slider(\"Top-p\", min_value=0.0, max_value=1.0, value=0.95, step=0.05)\n",
        "        temperature = st.slider(\"Temperature\", min_value=0.0, max_value=1.0, value=0.1, step=0.05)\n",
        "\n",
        "    if image_input and question_input:\n",
        "        image = Image.open(image_input).convert(\"RGB\")\n",
        "        image_np = np.array(image)\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            response = multimodal_understanding(image_np, question_input, default_seed, top_p, temperature)\n",
        "        st.text_area(\"Response\", value=response, height=200)\n",
        "    else:\n",
        "        st.info(\"Please upload an image and enter a question to start multimodal understanding.\")\n",
        "\n",
        "    # Examples\n",
        "    st.markdown(\"**Examples**\")\n",
        "    example_col1, example_col2 = st.columns(2)\n",
        "\n",
        "    with example_col1:\n",
        "        if st.button(\"Example 1: Explain this meme\"):\n",
        "            image_input = \"doge.png\"  # Replace with your actual example image path\n",
        "            question_input = \"Explain this meme\"\n",
        "            image = Image.open(image_input).convert(\"RGB\")\n",
        "            image_np = np.array(image)\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                response = multimodal_understanding(image_np, question_input, default_seed, top_p, temperature)\n",
        "            st.image(image, caption=\"Example Image\", use_column_width=True)\n",
        "            st.text_area(\"Response\", value=response, height=200)\n",
        "\n",
        "    with example_col2:\n",
        "        if st.button(\"Example 2: Convert the formula into latex code.\"):\n",
        "            image_input = \"equation.png\"  # Replace with your actual example image path\n",
        "            question_input = \"Convert the formula into latex code.\"\n",
        "            image = Image.open(image_input).convert(\"RGB\")\n",
        "            image_np = np.array(image)\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                response = multimodal_understanding(image_np, question_input, default_seed, top_p, temperature)\n",
        "            st.image(image, caption=\"Example Image\", use_column_width=True)\n",
        "            st.text_area(\"Response\", value=response, height=200)\n",
        "\n",
        "# --- Text-to-Image Generation Section ---\n",
        "with st.expander(\"**Text-to-Image Generation**\", expanded=True):\n",
        "    prompt_input = st.text_area(\"Enter your prompt for image generation\", height=150)\n",
        "    t2i_col1, t2i_col2 = st.columns(2)\n",
        "    with t2i_col1:\n",
        "        cfg_weight_input = st.slider(\"CFG Weight\", min_value=1.0, max_value=10.0, value=5.0, step=0.5)\n",
        "    with t2i_col2:\n",
        "        t2i_temperature = st.slider(\"T2I Temperature\", min_value=0.0, max_value=1.0, value=1.0, step=0.05)\n",
        "\n",
        "    if st.button(\"Generate Images\"):\n",
        "        if prompt_input:\n",
        "            with st.spinner(\"Generating images...\"):\n",
        "                images = generate_image(prompt_input, default_seed, cfg_weight_input, t2i_temperature)\n",
        "                st.image(images, width=256)\n",
        "        else:\n",
        "            st.warning(\"Please enter a prompt to generate images.\")\n",
        "\n",
        "    # Examples\n",
        "    st.markdown(\"**Examples**\")\n",
        "\n",
        "    example_prompts = [\n",
        "        \"Master shifu racoon wearing drip attire as a street gangster.\",\n",
        "        \"The face of a beautiful girl\",\n",
        "        \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n",
        "        \"A cute and adorable baby fox with big brown eyes, autumn leaves in the background enchanting,immortal,fluffy, shiny mane,Petals,fairyism,unreal engine 5 and Octane Render,highly detailed, photorealistic, cinematic, natural colors.\",\n",
        "        \"The image features an intricately designed eye set against a circular backdrop adorned with ornate swirl patterns that evoke both realism and surrealism. At the center of attention is a strikingly vivid blue iris surrounded by delicate veins radiating outward from the pupil to create depth and intensity. The eyelashes are long and dark, casting subtle shadows on the skin around them which appears smooth yet slightly textured as if aged or weathered over time.\\n\\nAbove the eye, there's a stone-like structure resembling part of classical architecture, adding layers of mystery and timeless elegance to the composition. This architectural element contrasts sharply but harmoniously with the organic curves surrounding it. Below the eye lies another decorative motif reminiscent of baroque artistry, further enhancing the overall sense of eternity encapsulated within each meticulously crafted detail. \\n\\nOverall, the atmosphere exudes a mysterious aura intertwined seamlessly with elements suggesting timelessness, achieved through the juxtaposition of realistic textures and surreal artistic flourishes. Each component\\u2014from the intricate designs framing the eye to the ancient-looking stone piece above\\u2014contributes uniquely towards creating a visually captivating tableau imbued with enigmatic allure.\"\n",
        "    ]\n",
        "\n",
        "    for i, example_prompt in enumerate(example_prompts):\n",
        "        if st.button(f\"Example {i+1}\"):\n",
        "            with st.spinner(\"Generating images...\"):\n",
        "                images = generate_image(example_prompt, default_seed, cfg_weight_input, t2i_temperature)\n",
        "                st.image(images, width=256)"
      ]
    }
  ]
}